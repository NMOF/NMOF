% \VignetteIndexEntry{Style Analysis with Differential Evolution}
% \VignetteKeyword{optimize}
\documentclass[a4paper]{article}
\usepackage[noae]{Sweave}
\usepackage{mathptmx}
\usepackage{natbib}
\usepackage{amsmath,amstext}
\usepackage[left = 2.5cm, top = 2cm, bottom = 3cm, right = 3.5cm]{geometry}
\usepackage{color}
\definecolor{grau2}{rgb}{.2,.2,.2}
\definecolor{grau7}{rgb}{.7,.7,.7}
% define *Sweave* layout
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{frame=single,xleftmargin=0em,%
  formatcom=\color{grau2},rulecolor=\color{grau7}}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
<<echo=false>>=
options(continue = "  ", digits = 5)
@
\begin{document}
{\raggedright{\LARGE Style Analysis with Differential Evolution}}\medskip

\noindent Enrico Schumann\\
\noindent \texttt{es@enricoschumann.net}\\
\bigskip


\nocite{Gilli2011b}


Style analysis, described in \cite{sharpe1992}, uses a
linear factor model to describe the returns of a
portfolio, with a few constraints added: the factors
should be actual asset classes; the factor loadings,
which are interpreted as weights, should sum to 100\%,
and should take on reasonable values.  For a typical
mutual fund, for instance, the weights should be
nonnegative.


\section{Data}

We generate random $X$ and $y$.  $X$ is a matrix that
would hold the observations on indices: rows are
observations; columns are indices.  All information is
collected in a list \texttt{Data}.

<<Data>>=
library("NMOF")
Data <- list(ni = 30,    ## number of indices
             no = 100,   ## number of observations
             maxi = 3,   ## maximum number of included indices
             sum2one = FALSE)   ## scale weights

Data$X <- array(rnorm(Data$no * Data$ni),
                dim = c(Data$no, Data$ni))
Data$y <- rnorm(Data$no)
str(Data)
@ 


\section{Least Squares}

We will use Differential Evolution to solve the model.
As a test, we compute the Least-Squares solution to
$y = X w + \epsilon$.  To minimise the variance of the
residuals, we just need to compute the variance of the
columns. Or, perhaps simpler, we just add a column of
ones to $X$ that is always included as a regressor. In
this way, the residuals will have a zero mean and Least
Squares is equivalent to minimising variance.

<<objective-fn>>=
ofun <- function(p, Data) {
  tmp <- Data$X %*% p - Data$y
  ##colSums(tmp * tmp)  ## for Least Squares
  apply(tmp, 2, var)
}

settings <- list(min = rep(0, Data$ni),
                 max = rep(1, Data$ni),
                 nG = 2000,  ## number of generations
                 loopOF = FALSE, printBar = FALSE)
sol <- DEopt(ofun, algo = settings, Data = Data)
@

To see whether we succeeded, we also compute the
solution with \texttt{lm}.  The differences between the
results are negligible (and could be made as small as
machine precision allows us to by increasing
\texttt{settings\$nG}).

<<>>=
## for Least Squares, fit 'Data$y ~ -1 + Data$X' instead
summary(abs(coef(lm(Data$y ~ Data$X))[-1] - sol$xbest))
@ 


\section{Constraints}

\label{sec:constraints}

We constrain the model as follows: all weights must be
nonnegative; the sum of the weights must equal one; and
there must be no more than \texttt{maxi} regressors
included in the model.

We handle these constraints with a repair function.
<<repair>>=
repair <- function(p, Data) {
  
  ## remove negative values
  p <- p + abs(p)
  
  ## reduce cardinality of solutions (if required)
  tmp <- p > 0
  cs <- colSums(tmp)
  for (i in which(cs > Data$maxi)) {
    nneg <- which(tmp[ ,i])
    p[sample(nneg, length(nneg) - Data$maxi), i] <- 0
  }
  
  ## sum = 1
  if (Data$sum2one) 
    for (i in seq_len(ncol(p)))
      p[ ,i] <- p[ ,i]/sum(p[ ,i])
  
  p
}
@ 

A test: we create a random population of \texttt{nP}
solutions and repair it.  The first three columns are
displayed.

<<test>>=
nP <- 200
pop <- array(rnorm(Data$ni * nP), dim = c(Data$ni, nP))

repair(pop, Data)[ ,1:3]
Data$sum2one <- TRUE
repair(pop, Data)[ ,1:3]
@ 

<<test-results>>=
settings <- list(min = rep(0, Data$ni),
                 max = rep(1, Data$ni),
                 nG = 2000,
                 nP = Data$ni * 5,
                 repair = repair,
                 loopOF = FALSE, loopRepair = FALSE, 
                 printBar = FALSE)
sol <- DEopt(ofun, algo = settings, Data = Data)
data.frame(row.names = which(sol$xbest > 0),
           weights = round(sol$xbest[sol$xbest > 0], 2))
@ 

We change the cardinality constraint.
<<>>=
Data$maxi <- 4
sol <- DEopt(ofun, algo = settings, Data = Data)
data.frame(row.names = which(sol$xbest > 0),
           weights = round(sol$xbest[sol$xbest > 0], 2))

@ 

\appendix

\section{QP solution}

See also \cite[p. 391]{Gilli2011b}.  
<<qp>>=
library("quadprog")
settings <- list(min = rep(0, Data$ni),
                 max = rep(1, Data$ni),
                 nG = 2000,
                 nP = Data$ni * 5,
                 repair = repair,
                 loopOF = FALSE, loopRepair = FALSE, 
                 printBar = FALSE)
Data$maxi <- Data$ni
sol <- DEopt(ofun, algo = settings, Data = Data)

D <- var(Data$X)
d <- cov(Data$y, Data$X)

min.w <- 0
AT <- rbind(1, diag(Data$ni))
b0 <- c(1, rep(min.w, Data$ni))
cbind(qp = round(solve.QP(D, d, t(AT), b0, meq = 1)$solution, 4),
      de = round(sol$xbest, 4))  
@ 


\section{More on variances and squared residuals}

Two examples for quadratic problems that do not require
a QP-solver.

\subsection{No QP needed, example 1: minimising a sum of squares}
  
In the linear regression model
\begin{align}
      y = X \beta + \epsilon
\end{align}
the method of Least Squares minimizes the sum of the
squared residuals $(y - X \beta)'(y - X \beta)$.
Analytically, we would solve the normal equations
\begin{align*}
     X'X\beta = X'y
\end{align*}
for $\beta$. Note that if $X$ contains a constant
column, the residuals will have zero mean, and hence
minimising the sum of squares is equivalent to
minimizing the variance of the residuals (see the next
example).
  
The sum of squares for a sample $X$ and $y$ can be written as:
\begin{align*}
     (y-X\beta)'(y-X\beta) & = y'y - y'X\beta - (X\beta)'y + (Xb)'X\beta \\
                           & =  y'y - 2 y'X\beta + b'X'X\beta
\end{align*}
(The product $y'X\beta$ is a scalar and hence equals
$\beta'X'y$.)  We drop $y'y$ since it does not depend
on $\beta$ and divide by 2 to obtain
\begin{align}
     - \underbrace{y'X}_{c}\beta + 
       \frac{1}{2}\beta'\underbrace{\vphantom{y}X'X}_{Q}\beta\,,
\end{align}
to be minimised. This looks exactly like the type of
model that a QP solver could handle.
  
  
\subsection{No QP needed, example 1: minimizing variance}
  
We stay with the regression model, but now we wish to
minimize the variance of the residuals. We regard every
row of $X$ as the realization of a random
variable. Hence, let $x$ be a random vector, then we
want to minimize
\begin{align*}
  \operatorname{var}(y-x'\beta) & = \operatorname{var}(y) + \phantom{\beta'} \operatorname{var}(x'\beta) -  2\operatorname{cov}(y,x'\beta) \\
                                & = \operatorname{var}(y) +          \beta'  \operatorname{var}(x')\beta -  2\operatorname{cov}(y,x')\beta
\end{align*}
We drop $\operatorname{var}(y)$ since it does not
depend on $\beta$, divide by 2, and obtain
\begin{align}
      - \operatorname{cov}(y,x')\beta + \frac{1}{2}\beta'\operatorname{var}(x')\beta \,.
\end{align}
For a sample of $X$ and $y$, this becomes
\begin{align}
     -  \underbrace{\operatorname{cov}(y,X)}_{c}\beta + \frac{1}{2}\beta'\underbrace{\operatorname{var}(X)}_{Q}\beta
\end{align}
with $\operatorname{cov}(y,X)$ a vector of sample
covariances between $y$ and the columns of $X$, and
$\operatorname{var}(X)$ the variance--covariance matrix
of the columns of $X$. We an solve it without a QP but
with techniques from linear regression. In =R=, for
instance, we can handle such problems with the function
\texttt{lm}\index{\texttt{lm}}.


<<quadprog>>=
library("quadprog")

## create data
na <- 10
ns <- 100
X  <- array(rnorm(ns*na), dim = c(ns,na))
y  <- rnorm(ns)

## minimise squares
### variant 1 -- linear regression
coef(lm(y ~ 0 + X))
#### variant 2 -- quadprog
Dmat <- crossprod(X) 
dvec <- y %*% X
Amat <- as.matrix(rep(0,na))
solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat)

## minimise variance
### variant 1 -- linear regression
coef(lm(y ~ X))[-1]
### variant 2 -- quadprog
Dmat <- cov(X) 
dvec <- cov(X,y)
Amat <- as.matrix(rep(0,na))
solve.QP(Dmat = Dmat, dvec = dvec,Amat = Amat)  
@ 


\bibliographystyle{plainnat}
\bibliography{NMOF}
\end{document}
