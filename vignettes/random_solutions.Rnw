% \VignetteIndexEntry{Random solutions}
% \VignetteKeyword{optimize}
\documentclass[a4paper]{article}
\usepackage[noae]{Sweave}
\usepackage{mathptmx}
\usepackage{natbib}
\usepackage{amsmath,amstext}
\usepackage[left = 2.5cm, top = 2cm, bottom = 3cm, right = 3.5cm]{geometry}
\usepackage{color}
\definecolor{grau2}{rgb}{.2,.2,.2}
\definecolor{grau7}{rgb}{.7,.7,.7}
% define *Sweave* layout
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{frame=single,xleftmargin=0em,%
  formatcom=\color{grau2},rulecolor=\color{grau7}}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
<<echo=false>>=
options(continue = "  ", digits = 5)
@
\begin{document}
{\raggedright{\LARGE Random solutions}}\medskip

\noindent Enrico Schumann\\
\noindent \texttt{es@enricoschumann.net}\\
\bigskip


\nocite{Gilli2011b}

\section{Random solutions}

A na{\"\i}ve approach to solving an optimisation model
could be this: randomly generate a large number of
candidate solutions; evaluate all solutions; and pick
the best one.  This best solution is the overall
solution.  Now suppose we repeated the whole procedure
a second time.  Most likely the overall solution would
differ from the first one.  In other words, the
solution $x$ that we obtain through our sampling
strategy is stochastic.  The difference between our
solution and the actual optimum would be a kind of
truncation error: provided our sampling procedure
covered the whole space of all possible solutions, if
we sampled more and more, we should in principle come
arbitrarily close to the optimum. Importantly, the
variability of the solution stems from the numerical
technique that we used to solve the model; it has
nothing to do with any error terms that we may have in
the model (such as the error in a regression).

From the viewpoint of the user of optimisation software
(let us call him the Analyst), stochastic solutions may
even occur with non-stochastic methods: think of search
spaces with many local minima or other badly-behaved
features.  Even if the Analyst used a deterministic
method like a gradient search, repeated runs from
different starting points result in different
solutions.

The Analyst who runs the optimisation may thus consider
the result of an optimization procedure as a random
variable with some distribution $D$. What exactly the
``result'' is, depends: We will want to look at the
objective function value (i.e., the solution quality),
but we may also look at the decision variables given by
a solution.[*] In any case, we collect all the
quantities of interest in a vector $R$ (as in
result). The result $R_i$ of a restart $i$ is a random
draw from $D$.  The trouble is that we do not know what
$D$ looks like.  But fortunately, there is a simple way
to find out for a given model. We run a reasonably
large number of restarts and each time store
$R_i$. Finally, we compute the empirical distribution
function of the $R_i$, $i = 1$, \ldots,
number-of-restarts as an estimate of $D$.

For simplicity, let us only look the at the objective
function value that is associated with a given
solution, i.e. $R_j$ stores the solution quality.  For
a given model or model class, the shape of the
distribution D will depend on the chosen method. Some
techniques will be more appropriate than others and
give less variable and on average better results. And D
will often depend on the particular settings of the
method, most importantly the number of iterations --
the search time -- that we allow.  Unlike classical
optimization techniques, heuristics can escape from
local minima; they will not necessarily get trapped.
So if we let the algorithm search for longer, we can
hope to find better solutions. For minimization
problems, when we increase the number of iterations,
the mass of D will move to the left and the
distribution will become less variable. Ideally, with
more iterations, D should degenerate into a single
point, the global minimum.


There exist proofs of this convergence to the global
minimum for many heuristic methods (see GMS for
references).  Unfortunately, these proofs are not much
help for practical applications. They often rely on
asymptotic arguments; and in some cases they are
nonconstructive (e.g., Alth\"ofer and Koschnick, 1991,
for Threshold Accepting): they demonstrate that
parameter settings exist that lead (asymptotically) to
the global optimum. Yet, practically, there is no way
of telling whether the chosen parameter setting is
correct in this sense; we are never guaranteed that D
really degenerates to the global optimum as the number
of iterations grows.  Fortunately, we do not need these
proofs to make meaningful statements about the
performance of particular methods.  For a given model
class, we can run experiments. Such experiments also
help investigate the sensitivity of the solutions with
respect to different parameter settings for the
heuristic.  Experimental results are of course no proof
of the general appropriateness of a method, but they
are evidence of how a method performs for a given class
of models;[**] often this is all that is needed for
practical applications.





To be clear: if you are serious



[*] Side note: that means that D is multidimensional, but we may
    prefer to look only from one margin.

    [**] However, just because we may not be able to
    prove something does not mean that it is not the
    case: a wonderful illustration of this "no
    theoretical proof, but empirical evidence" is
    Goldbach's conjecture. In its best-known form, it
    states that every even number greater than two is
    the sum of two prime numbers. It is a conjecture
    because to this day, it has not been proved in all
    generality. But it has been brute-force--tested for
    even extremely large numbers, and it held up. Thus:
    one can have empirical evidence that a method works
    well, even if one cannot prove its optimality.

\section{An example}


\bibliographystyle{plainnat}
\bibliography{NMOF}
\end{document}
