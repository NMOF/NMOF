\documentclass[11pt]{article}

\title{Distributed computations with the NMOF package}
\author{Enrico Schumann\\\footnotesize\href{mailto:es@enricoschumann.net}{\texttt{es@enricoschumann.net}}}
\date{2014-10-23}

% *LANGUAGE/ENCODING*
\usepackage[british]{babel}

% *LAYOUT*
\usepackage[left = 2.5cm,top = 3cm, bottom = 3cm, right = 4.5cm]{geometry}
\renewcommand{\figurename}{Figure}
\usepackage{algorithmic,algorithm}
\usepackage{framed}

% *FONTS*
%\usepackage{mathptmx}
%\usepackage[scaled=0.90]{helvet}
%\usepackage{courier}%
\usepackage{sfourier}
\renewcommand{\rmdefault}{fmnj}
\newcommand{\hy}{{\fontencoding{OT1}\fontfamily{pcr}\selectfont - }}



% soul definition
\usepackage{soul}
\capsdef{T1/ppl/m/n/5-15}{\scshape}{.16em}{.55em}{.2em}

%\newcommand{\Name}[Anzahl]{Definition}
\newcommand{\TA}{\caps{ta}}
\newcommand{\DE}{\caps{de}}
\newcommand{\GA}{\caps{ga}}
\newcommand{\PS}{\caps{ps}}
\newcommand{\R}{\textsf{R}}
\newcommand{\Sfour}{\caps{s}4}
\newcommand{\nmof}{\caps{nmof}}
\newcommand{\gms}{\caps{gms}}

% *MATHS*
\usepackage{amsmath}

% *COLOR/GRAPHICS*
\usepackage{color}
\usepackage{graphics,pst-node,pst-plot,pstricks,pst-fun}
\definecolor{grau2}{rgb}{.2,.2,.2}
\definecolor{grau7}{rgb}{.7,.7,.7}

% *BIB*
\usepackage[round]{natbib}
\usepackage[noae,nogin]{Sweave}
\SweaveOpts{keep.source=TRUE,eps=TRUE,prefix.string=figures/fig}
% *INDEX*
\usepackage{makeidx}\makeindex %\index{xxxx}, \index{xxx!yyy}


% *HYPERREF* settings
\usepackage{hyperref}
\hypersetup{%
colorlinks = false,%
breaklinks = true,%
pdfborder={0 0 0},%
pdftitle={Distributed computations with the NMOF package},%
pdfauthor={Enrico Schumann},%
pdfsubject={finance, heuristic optimisation in finance,%
numerical methods in finance, distributed computing},%
pdfkeywords={finance, heuristic optimisation}}


% define *Sweave* layout
\DefineVerbatimEnvironment{Sinput}{Verbatim}{}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{frame=single,xleftmargin=0em,%
formatcom=\color{grau2},rulecolor=\color{grau7}}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
<<echo=false>>=
options(continue = " ", digits = 3, width = 65)
@
\begin{document}
\maketitle
\tableofcontents

%% \begin{framed}
%%   \noindent I am currently in the process of checking and updating
%%   these examples for package \texttt{parallel}, as it supersedes
%%   package \texttt{multicore}.  I would be happy about comments and
%%   corrections. (Note that I still refer to it as \texttt{multicore},
%%   but the code uses package \texttt{parallel}.)
%% \end{framed}

\section{Overview}

This manuscript contains several examples of distributed computations
-- more specifically, using several cores on one machine -- in the
\nmof\ package \citep*{Gilli2011b}.  Such computations are supported
in the functions \texttt{bracketing}, \texttt{GAopt},
\texttt{gridSearch}, \texttt{restartOpt} through package
\texttt{parallel}.  (Before, packages \texttt{multicore}
\cite{Urbanek2011} and \texttt{snow} \cite{Tierney2011} were
supported.)  This report mainly serves as a convenient collection of
test cases for these functions. I am grateful for comments and
corrections.

The latest version of the package is available from

\url{http://enricoschumann.net/R/packages/NMOF/index.htm}\,; 

\noindent the package is also available from \caps{cran}.  To install
the package from within \R, type
<<eval = FALSE>>=
install.packages("NMOF") ## CRAN
install.packages("NMOF", repos = "http://enricoschumann.net/R")
@
to download and install it.  For all examples to follow, the package needs to be
attached.
<<>>=
require("NMOF")
set.seed(1122344)
nC <- 4L ## the number of cores to be used
@
The distributed computations rely on package \textsf{parallel}.   We
will use package
\textsf{rbenchmark} \citep{Kusnierczyk2010} to measure the time that
particular computations need.
<<>>=
require("rbenchmark")
require("parallel")
@
This report is written with Sweave \citep{Leisch2002}.  The code is
part of the package; it can be found in the subdirectory
\texttt{NMOFex}.   To show the code in \R, you can use the function
\texttt{system.file}.
<<eval = FALSE>>=
whereToLook <- system.file("NMOFex/NMOFdist.R", package = "NMOF")
file.show(whereToLook, title = "NMOF examples")
@
The latest pdf version of this report can be obtained from

\url{http://enricoschumann.net/NMOF.htm}

\section{Simple tests}

\subsection{Slowing things down}

We first run a simple test to see if a specific function is expensive
enough for distributed computation.
<<>>=
testFun <- function(ignore, delay) {
    Sys.sleep(delay)
    1
}
 
delay <- 0.05     ## running time of function
n <- 8            ## how many calls per lapply
repl <- 10        ## how many restarts
sq <- seq_len(n)
@
Note that we set up the cluster for \texttt{clusterApply} before we
measure the computing time.
<<>>=
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
benchmark(lapply(sq, testFun, delay),                ## serial
          mclapply(sq, testFun, delay),              ## formerly 'multicore'
          clusterApply(cl, sq, testFun, delay),      ## formerly 'snow'
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = repl)
stopCluster(cl)
@

\subsection{Copying arguments}

With \texttt{clusterApply}, it can be beneficial to copy constant function
arguments to the nodes.  Suppose we have a Least-Squares objective
function.
<<>>=
OF <- function(b, X, y) {
    temp <- X %*% b - y
    sum(temp^2)
}
@ 
For this function, \texttt{b} are the parameters to be optimised, but
\texttt{X} and \texttt{y} stay fixed.  

We run an experiment.  We start by creating random \texttt{X},
\texttt{y} and \texttt{b}.
<<>>=
ncol <- 10; nrow <- 200
X <- array(rnorm(nrow * ncol), dim = c(nrow, ncol))
y <- rnorm(nrow)
b <- rnorm(ncol)
@ 

Just a test.
<<>>=
OF(b, X, y)
@ 

Suppose we used such a function in a multiple-solution heuristic such
as~DE.  We assume the population \texttt{lP} has size \texttt{n}.
<<>>=
n <- 50            ## how many calls per lapply
sq <- seq_len(n)
lP <- vector("list", length = n)
for (i in sq)
    lP[[i]] <- b
@

Next we define various expressions.
<<>>=
snow_with_copying <- expression({
    ignore1 <- clusterApply(cl, lP, OF, X, y)  
})

lapply <- expression({
    ignore2 <- lapply(lP, OF, X, y)  
})

snow_without_copying <- expression({
    OF1 <- function(b) {
        temp <- X %*% b - y
        sum(temp^2)
    }
    ignore3 <- clusterApply(cl, lP, OF1)
})

cl <- makeCluster(rep("localhost", nC), type = "SOCK")
clusterExport(cl, list("X", "y"))
benchmark(lapply,            
          snow_with_copying, 
          snow_without_copying,
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 50)
stopCluster(cl)
@ 
Both parallel computations are slow, as the objective function is not
expensive enough.  But parallel without copying is faster, nevertheless.


We compare the results.
<<>>=
all.equal(ignore1, ignore2)
all.equal(ignore2, ignore3)
@


\section{\texttt{bracketing}}

We repeat the example from \citet[p.~290]{Gilli2011b}; again we
add a small delay to the function.

<<>>=
testFun <- function(x) {
    Sys.sleep(0.1)
    cos(1/x^2)
}
with_loop <- expression(
    sol1 <- bracketing(testFun,
                       interval = c(0.3, 0.9),
                       n = 100L))

with_multicore <- expression(
    sol2 <- bracketing(testFun,
                       interval = c(0.3, 0.9),
                       n = 100L,
                       method = "multicore", 
                       mc.control = list(mc.cores = nC)))

with_snow  <- expression(
    sol3 <- bracketing(testFun,
                       interval = c(0.3, 0.9),
                       n = 100L, method = "snow", cl = nC))

benchmark(with_loop, 
          with_multicore, 
          with_snow,
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 1)
@

We check.
<<>>=
all.equal(sol1, sol2)
all.equal(sol1, sol3)
@ 


\section{Multiple-solution (a.k.a.\/ population-based) optimisation methods}

\subsection{Evaluting several solutions -- a prototype}

We create a population \texttt{P} of solution vectors, shaped as a
matrix in which every column is one solution.
<<>>=
ncol <- 20
nrow <- 1000
P <- array(rnorm(nrow * ncol), dim = c(nrow, ncol))
@

As an example, we define a simple objective function, which partially
sorts a vector \texttt{x}.  Such a function could be
used, for instance, in a Value-at-Risk calculation, or for some kind of
robust statistic.
<<>>=
fun <- function (x, h)
    sort(x, partial = h)[h]
@

We check the function on the first column of \texttt{P}.
<<>>=
h <- 5L
fun(P[ ,1L], h)
@

The most natural way to evaluate all solutions is to loop over the
columns of \texttt{P}, for which we create a new function
\texttt{loopfun}.  The function takes as arguments a matrix
\texttt{x} and a function \texttt{f}, to be applied to the columns of
\texttt{x}.  Further arguments to \texttt{f} are passed through \texttt{...}.
<<>>=
loopfun <- function(x, f, ...) {
    ns <- ncol(x)
    fv <- numeric(ns)
    for (i in seq_len(ns))
        fv[i] <- f(x[ ,i], ...)
    fv
}
@

With this function, we can evaluate the whole population, not just a
single column.
<<>>=
loopresult <- loopfun(P, fun, h)
@

Now, how to exploit the fact that we can evaluate the columns of
\texttt{P} in any order?  That is, how to distribute the computations?
The simplest way is to call a member of the apply family, of the sort
that comes with packages \texttt{parallel}; we will
use \texttt{clusterApply} here.

First, we create a list from the columns of \texttt{P}.
<<>>=
mat2list <- function(x) {
    nx <- ncol(x)
    listP <- vector(mode = "list", length = nx)
    for (s in seq_len(nx))
        listP[[s]] <- P[ ,s]
    listP
}
listP <- mat2list(P)
@

This could more have been written more compactly as \texttt{split(P,
  col(P))}; but the loop version is faster.  So, we can compare
the results of \texttt{loopfun} with a distributed computation.

<<>>=
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
snowresult <- unlist(clusterApply(cl, listP, fun, h))
stopCluster(cl)
all.equal(loopresult, snowresult)
@

We can check the running time. We set up the cluster outside the test;
we also leave out the call to \texttt{unlist}.
<<>>=
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
benchmark(clusterApply(cl, listP, fun, h),
          loopfun(P, fun, h),
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 100)
stopCluster(cl)
@

Using \texttt{clusterApply} is much slower; so, apparently, 
\texttt{fun} is far too cheap to benefit from
distribution.  But how about \texttt{loopfun}?  That is, why not split
the matrix \texttt{P} into smaller submatrices, and then let the nodes
loop over these submatrices.

We make our matrix larger (more columns).
<<>>=
ncol <- 100
nrow <- 1000
P <- array(rnorm(nrow * ncol), dim = c(nrow, ncol))

system.time(for (i in seq_len(10000L)) fun(P[ ,1L], 10L))
@

The number of columns per core.
<<>>=
d <- round(ncol/nC) ## nC is the number of cores
@ 
We split our matrix into \texttt{d} parts.
<<>>=
listP <- vector(mode = "list", length = nC)
for (s in seq_len(nC))
    listP[[s]] <- P[ ,(d*s-d+1):min(ncol, d*s)]
@ 
Finally, we run a test.

<<>>=
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
benchmark(parallel.result <- clusterApply(cl, listP, loopfun, fun, h),
          loop.result <- loopfun(P, fun, h),
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 100)
stopCluster(cl)
all.equal(loop.result, unlist(parallel.result))
@
Thus, the distributed computation is still slower than the simple
loop, but we have improved massively compared with the first approach
of sending single solutions to the nodes.

%% TODO: check if parallel always slower


\subsection{\texttt{GAopt}}

\texttt{GAopt} currently supports distributed evaluation of the
objective function (in future versions, the \texttt{repair} and
\texttt{penalty} functions may also be distributed).

We use the matching example from \texttt{?GAopt},

<<>>=
OF <- function(x, y) {
    Sys.sleep(0.001)
    sum(x != y)
}
size <- 20L            ## the length of the string
y <- runif(size) > 0.5 ## the true solution
with_loop <- list(nB = size, nP = 200L, nG = 50L, prob = 0.002,
                  printBar = FALSE, printDetail = FALSE,
                  methodOF = "loop")
with_snow <- list(nB = size, nP = 200L, nG = 50L, prob = 0.002,
                  printBar = FALSE, printDetail = FALSE,
                  methodOF = "snow", cl = nC)
with_multicore <- list(nB = size, nP = 200L, nG = 50L, prob = 0.002,
                       printBar = FALSE, printDetail = FALSE,
                       methodOF = "multicore")

benchmark(GAopt(OF, algo = with_loop, y = y),
          GAopt (OF, algo = with_snow, y = y),
          GAopt(OF, algo = with_multicore, y = y),
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 1)

@

To pass optional arguments to \texttt{mclapply},
we need to collect them in a list \texttt{mc.control}, which needs to
be added to \texttt{algo}.  As an example, we instruct
\texttt{parallel} to use just one core; thus, we should see no
speedup.
<<>>=
with_multicore$mc.control <- list(mc.cores = 1L)
## system.time(GAopt(OF, algo = with_multicore, y = y))
benchmark(GAopt(OF, algo = with_loop, y = y),
          GAopt(OF, algo = with_multicore, y = y),
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 1)
@

A few more tests.
<<>>=
OF <- function(x, y) {
    Sys.sleep(0.01)
    sum(x != y)
}
size <- 10L; y <- runif(size) > 0.5
algo <- list(nB = size, nP = 20L, nG = 100L, prob = 0.002,
             printBar = FALSE, methodOF = "loop")
t1 <- system.time(sol <- GAopt(OF, algo = algo, y = y))
all.equal(sol$xbest, y)
all.equal(sol$OFvalue, 0)

algo <- list(nB = size, nP = 20L, nG = 100L, prob = 0.002,
             printBar = FALSE, methodOF = "snow", cl = nC)
t2 <- system.time(sol <- GAopt(OF, algo = algo, y = y))
all.equal(sol$xbest, y)
all.equal(sol$OFvalue, 0)
@

This allows us to check the speedup (but from only one replication).
<<>>=
round(t1[[3L]]/t2[[3L]],1)
@

We can also pass further parameters to the objective function.
<<>>=
OF <- function(x, y, k) {
    Sys.sleep(0.01)
    sum(x != y) + k
}
size <- 10L; y <- runif(size) > 0.5; k <- 10
algo <- list(nB = size, nP = 20L, nG = 100L, prob = 0.002,
             printBar = FALSE, printDetail = FALSE,
             methodOF = "loop")
t1 <- system.time(sol <- GAopt(OF, algo = algo, y = y, k = k))
all.equal(sol$xbest, y)
all.equal(sol$OFvalue, k)

algo <- list(nB = size, nP = 20L, nG = 100L, prob = 0.002,
             printBar = FALSE, printDetail = FALSE,
             methodOF = "snow", cl = nC)
t2 <- system.time(sol <- GAopt(OF, algo = algo, y = y, k = k))
all.equal(sol$xbest,y)
all.equal(sol$OFvalue, k)
@

\section{\texttt{gridSearch}}

We use a simple test function.
<<>>=
testFun  <- function(x) {
    Sys.sleep(0.1)
    x[1L] + x[2L]^2
}
lower <- 1:2; upper <- 5; n <- 10
with_loop <- expression(
    sol1 <- gridSearch(fun = testFun,
                       lower = lower, upper = upper,
                       n = n, printDetail = FALSE))

with_multicore <- expression(
    sol2 <- gridSearch(fun = testFun,
                       lower = lower, upper = upper,
                       n = n, printDetail = FALSE,
                       method = "multicore"))

with_snow <- expression(
    sol3 <- gridSearch(fun = testFun,
                       lower = lower, upper = upper,
                       n = n, printDetail = FALSE,
                       method = "snow",
                       cl = nC))

benchmark(with_loop, with_multicore, with_snow,
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 1)
all.equal(sol1, sol2)
all.equal(sol1, sol3)
all.equal(sol3$minlevels, 1:2)
@

This test function may also need additional arguments. Here we pass a
variable \texttt{k}.
<<>>=
testFun  <- function(x, k) {
    Sys.sleep(0.1)
    x[1L] + x[2L]^2 + k
}
lower <- 1:2; upper <- 5; n <- 5; k <- 1
sol1 <- gridSearch(fun = testFun, k = k,
                   lower = lower, upper = upper,
                   n = n, printDetail = FALSE)
sol2 <- gridSearch(fun = testFun,k = k,
                   lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "multicore")
sol3 <- gridSearch(fun = testFun,k = k,
                   lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "snow", cl = nC)
all.equal(sol1, sol2)
all.equal(sol1, sol3)
all.equal(sol3$minlevels, 1:2)
@

To pass optional arguments to \texttt{parallel}'s \texttt{mclapply},
we need to collect them in a list \texttt{mc.control}, which needs to
be added to \texttt{algo}. Here we set a seed.
<<>>=
testFun  <- function(x) {
    Sys.sleep(0.1)
    x[1L] + x[2L] + runif(1)
}
lower <- 1:2; upper <- 5; n <- 3
set.seed(5)
sol2 <- gridSearch(fun = testFun,
                   lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "multicore",
                   mc.control = list(mc.set.seed = FALSE))
temp <- sol2$values
set.seed(5)
sol2 <- gridSearch(fun = testFun,
                   lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "multicore",
                   mc.control = list(mc.set.seed = FALSE))
all.equal(sol2$values, temp)
@

Setting a seed is also possible with method \texttt{snow}. 

<<>>=
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
clusterSetRNGStream(cl, 2222)
sol3 <- gridSearch(fun = testFun, lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "snow", cl = cl)
stopCluster(cl)
temp <- sol3$values

## ... and again
cl <- makeCluster(c(rep("localhost", nC)), type = "SOCK")
clusterSetRNGStream (cl, 2222)
sol3 <- gridSearch(fun = testFun, lower = lower, upper = upper,
                   n = n, printDetail = FALSE,
                   method = "snow", cl = cl)
stopCluster(cl)
all.equal(sol3$values, temp)
@

\section{\texttt{restartOpt}}

We test with \texttt{TAopt} and a toy problem: find a
numeric vector~\texttt{x} that matches another numeric vector
\texttt{xTRUE} through randomly changing \texttt{x}.
<<>>=
xTRUE <- runif(5L)
data <- list(xTRUE = xTRUE,  ## the TRUE solution
             step = 0.02     ## step size for neighbourhood
             )
OF <- function(x, data)
    max(abs(x - data$xTRUE))
neighbour <- function(x, data)
    x + runif(length(data$xTRUE))*data$step - data$step/2
x0 <- runif(5L)              ## a random starting solution
algo <- list(q = 0.05, nS = 200L, nT = 10L,
             neighbour = neighbour, x0 = x0,
             printBar = FALSE, printDetail = FALSE)
@
Now we call \texttt{restartOpt}.
<<>>=
with_loop <- expression(
    sols1 <- restartOpt(fun = TAopt, n = 100L,
                        OF = OF, algo = algo, data = data))

with_multicore <- expression(
    sols2 <- restartOpt(fun = TAopt, n = 100L,
                        OF = OF, algo = algo, data = data,
                        method = "multicore"))

with_snow <- expression(
    sols3 <- restartOpt(fun = TAopt, n = 100L,
                        OF = OF, algo = algo, data = data,
                        method = "snow", cl = nC))

benchmark(with_loop, with_multicore, with_snow,
          columns = c("test", "elapsed", "relative"),
          order = "relative", replications = 1)
all.equal(length(sols1), 100L)
all.equal(length(sols2), 100L)
all.equal(length(sols3), 100L)
@

\newpage
\appendix
\section{Resources}

You can download all the book's code examples from the book's home
page,\medskip

\url{http://nmof.net} \bigskip

\noindent The latest version of the \nmof\ package is available
from\medskip

\url{http://enricoschumann.net/R/packages/NMOF/index.htm}\bigskip

\noindent but note that this is the development version. More stable
versions are available from CRAN. \bigskip

\noindent New versions of the package and other news are announced
through the \texttt{NMOF-news} mailing list; to browse the archives or
to subscribe, go to\medskip

\url{https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/nmof-news}

\section{Package version}
<<results=tex>>=
toLatex(sessionInfo())
@

\bibliographystyle{plainnat}
\bibliography{Library,MLibrary}


\end{document}
